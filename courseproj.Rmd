---
title: "courseproj"
author: "Makenzie Barber"
date: "2022-08-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#importdata
REALE <- read.csv("data.csv")
```


```{r}
#installlibraries
library(caret)
library(regclass)
library(dplyr)
library(glmnet)
library(pROC)
library(knitr)
library(kableExtra)
library(randomForest)
```

```{r}
suggest_transformation <- function(x,powers=seq(0,3,by=0.5),add=0) {
require(robustbase)
if(add!=0) { x <- x + add }
if(min(x)<=0) { powers <- powers[which(powers>0)] }
skewnesses <- rep(0,length(powers))
for (p in powers) {
if(p==0) { x.trans <- log10(x) } else { x.trans <- x^p }
skewnesses[which(powers==p)] <- mc(x.trans)
}
best.p <- powers[which.min(skewnesses)]
if(best.p==0) { return("log10")} else { return(best.p) }
}
```


```{r}
#data clean up

#omit any nas 
#AND
#Find correlation between columns 
cor.between.columns <- cor(na.omit(REALE)
)
highcorr <- findCorrelation(cor.between.columns, cutoff = .90, verbose = FALSE, names = TRUE, exact = FALSE) #Store highly correlated column name into a vector for ease of removal

COR <- head(sort(abs(cor.between.columns['TAX',]), decreasing=TRUE)) #store matrix into a vector for table purposes
  
#transformations check

suggest_transformation(REALE$MEDV)


NearZero <- nearZeroVar(REALE) #None near zero variance found!

REALE <- select(REALE, -highcorr) #Remove highly correlated column

```


```{r}
#trainholdout
set.seed(474) #keeps numbers consistent
train.rows <- sample(1:nrow(REALE), 0.4*nrow(REALE)) #setting train rows using 40% of data
train <- REALE[train.rows,] #creating train data
holdout <- REALE[-train.rows,] #creating holdout data without the train rows



mean(train$MEDV) #comparing means before modeling
mean(holdout$MEDV)

```

```{r}
#firstmodel
trControl <- trainControl(method = "cv", number = 3) #training the model

set.seed(474)
gl <- train(MEDV~., data=train, method='glm',trControl=trControl, preProc = c("center"), na.action = na.roughfix) #glm model


gl$results


#predictions for this model
#holding data
actual <- train$MEDV
#creating predictors
pred <- predict(gl, newdata=train)

#actual predictions :)
PRED <- postResample(predict.train(gl, newdata=holdout,na.action = na.roughfix),actual)
PRED %>%
  kbl(digits = 2)
```


```{r}
#secondmodel
set.seed(474)
#new grid to control a GLMNet model
tControl <- trainControl(method="repeatedcv", number=3)

#tuningparameters
tuningG <- expand.grid(alpha= seq(0,1, by=.5),lambda=10^seq(-1,2,by=.5))
set.seed(474)

#model
glmn <- train(MEDV~., data=train,method='glmnet', trControl=tControl,tuneGrid=tuningG,preProc="center", na.action=na.roughfix)

#find the lowest RMSE
GLNM <- glmn$results[which.min(glmn$results$RMSE), ]
#checkingbesttune
glmn$bestTune[rownames(glmn$bestTune), ]
#RMSE on holdout
GLMNP <- postResample(predict.train(glmn,newdata = holdout,na.action=na.roughfix),holdout$MEDV)
```


```{r}
#thirdmodel
#rf

#tuning parameters
forestGrid <- expand.grid(mtry=c(1,3,9))
set.seed(474)

#model
forest <- train(MEDV~., data=train, method="rf", preProc=c("center"),trControl=tControl, tuneGrid=forestGrid, verboseIter=TRUE, na.action=na.roughfix)

#check results
fores <- forest$results[which.min(forest$results$RMSE),] 
#validate predictions
fore <- forest$results[rownames(forest$bestTune),]

varImp(forest) #variable importance

```


```{r}
#makenicetables
COR %>%
  kbl(digits = 2) %>% 
  kable_classic()

PRED %>%
  kbl(digits = 2) %>%
  kable_classic_2()

GLNM %>%
  kbl(digits=2) %>%
  kable_classic()

GLMNP %>%
  kbl(digits=2) %>%
  kable_classic_2()

fores %>%
  kbl(digits=2) %>%
  kable_classic()


fore %>%
  kbl(digits=2) %>%
  kable_classic_2()
```

